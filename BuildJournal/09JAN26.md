# Build Journal - January 9, 2026

In this build journal, document all steps done with reasons why those steps were taken. Ensure that everything document has the date and time.

---

## Phase 1: Pose Detection Foundation - Initial Implementation

**Date:** January 9, 2026  
**Time:** ~20:00 PST

### Step 1: Create Pose Type Definitions
**Time:** ~19:50 PST

**Action:** Created comprehensive TypeScript type definitions for pose detection.

**Files Created:**
- `src/types/pose.ts` - Type definitions for pose detection

**Contents:**
- `KeypointIndex` enum - 17 keypoint indices (nose, eyes, ears, shoulders, elbows, wrists, hips, knees, ankles)
- `Keypoint` interface - Single keypoint with x, y, score
- `Pose` interface - Full pose detection result with keypoints array and overall score
- `SkeletonConnection` interface - Connection between two keypoints
- `SKELETON_CONNECTIONS` constant - Predefined skeleton connections for drawing

**Reason:** Strong typing is essential for TypeScript projects. These types will be used throughout the pose detection pipeline and ensure type safety when working with MoveNet output.

**Key Design Decisions:**
- Used enum for keypoint indices for better code readability
- Separated keypoint data structure from skeleton connections
- Included confidence scores in keypoint interface for filtering low-confidence detections

---

### Step 2: Implement Pose Detection Service
**Time:** ~19:52 PST

**Action:** Created the core pose detection service using TensorFlow.js and MoveNet Lightning.

**Files Created:**
- `src/services/poseDetection.ts` - Pose detection service

**Implementation Details:**
- Singleton pattern for service instance
- Async initialization method that loads MoveNet Lightning model from TensorFlow Hub
- Model URL: `https://tfhub.dev/google/tfjs-model/movenet/singlepose/lightning/4`
- Input size: 256x256 pixels (MoveNet Lightning requirement)
- Minimum keypoint confidence threshold: 0.3
- Proper tensor cleanup to prevent memory leaks

**Key Features:**
- `initialize()` - Loads TensorFlow.js and MoveNet model
- `isReady()` - Checks if service is initialized
- `detectPose(imageTensor)` - Processes image tensor and returns pose
- `dispose()` - Cleans up model resources

**Reason:** Centralized service for pose detection allows for easy testing, maintenance, and potential future optimizations. Singleton pattern ensures only one model instance is loaded.

**Technical Notes:**
- MoveNet Lightning chosen for speed (30+ FPS target)
- Model loads from TensorFlow Hub (no local bundling needed)
- Tensor cleanup is critical to prevent memory leaks in React Native
- Keypoints are filtered by confidence score before returning

**Challenges:**
- MoveNet outputs keypoints in format [y, x, score] - need to parse correctly
- Need to handle model loading errors gracefully
- Tensor disposal must be done carefully to avoid memory issues

---

### Step 3: Install React Native SVG
**Time:** ~19:54 PST

**Action:** Installed react-native-svg package for drawing keypoints and skeleton.

**Command:**
```bash
npm install react-native-svg --legacy-peer-deps
```

**Package Installed:**
- `react-native-svg@^15.15.1`

**Reason:** React Native SVG is the standard library for drawing vector graphics in React Native. Needed for drawing keypoints (circles) and skeleton connections (lines) on the camera overlay.

---

### Step 4: Create PoseOverlay Component
**Time:** ~19:55 PST

**Action:** Created component to visualize pose keypoints and skeleton on camera feed.

**Files Created:**
- `src/components/PoseOverlay.tsx` - Pose visualization overlay

**Implementation Details:**
- Uses React Native SVG for rendering
- Draws 17 keypoints as green circles
- Draws skeleton connections as green lines
- Scales coordinates from model input size (256x256) to screen coordinates
- Only renders keypoints with confidence > 0
- Opacity based on keypoint confidence scores

**Key Features:**
- Absolute positioning overlay (doesn't interfere with touch events)
- Dynamic scaling based on camera dimensions
- Visual feedback with opacity based on confidence
- Green color scheme for visibility

**Reason:** Visual feedback is essential for users to see that pose detection is working. Overlay approach allows pose visualization without blocking camera view.

**Design Decisions:**
- Green color chosen for good visibility on most backgrounds
- Keypoint radius: 4 pixels (visible but not obtrusive)
- Skeleton line width: 2 pixels
- Opacity tied to confidence for visual feedback on detection quality

---

### Step 5: Create CameraScreen
**Time:** ~19:58 PST

**Action:** Created main camera screen with pose detection integration.

**Files Created:**
- `src/screens/CameraScreen.tsx` - Camera screen with pose detection

**Implementation Details:**
- Uses Expo Camera (`CameraView` component)
- Camera permissions handling with user-friendly UI
- TensorFlow.js initialization on mount
- MoveNet model loading with loading states
- FPS counter for performance monitoring
- Pose confidence score display
- Error handling with user-friendly messages

**Key Features:**
- Permission request flow
- Loading states during model initialization
- Error handling and display
- FPS counter (updates every second)
- Pose confidence indicator
- Frame processing setup (placeholder for now)

**Reason:** CameraScreen is the core user interface for pose detection. Needs to handle all edge cases (permissions, loading, errors) gracefully.

**Technical Notes:**
- Camera aspect ratio: 4:3 (standard)
- Target FPS: 30 frames per second
- Frame interval: ~33ms between frames
- Front-facing camera by default (better for workout form analysis)

**Current Limitation:**
- Frame processing is currently a placeholder
- Need to implement actual frame capture from camera feed
- Will require expo-gl texture or alternative frame capture method

---

### Step 6: Update Navigation
**Time:** ~20:00 PST

**Action:** Added CameraScreen to navigation and updated HomeScreen with navigation button.

**Files Modified:**
- `App.tsx` - Added CameraScreen to Stack Navigator
- `src/screens/HomeScreen.tsx` - Added "Start Camera" button with navigation

**Changes:**
- Added Camera route to Stack Navigator
- Created navigation button on HomeScreen
- Styled button with green theme matching app design

**Reason:** Users need a way to access the camera screen. HomeScreen serves as the entry point with clear call-to-action.

**UI Design:**
- Green button (#00FF00) matching pose overlay color
- Bold text for visibility
- Rounded corners for modern look
- Proper spacing and margins

---

## Current Status

### Completed Tasks:
- ✅ Created pose type definitions
- ✅ Implemented pose detection service
- ✅ Installed React Native SVG
- ✅ Created PoseOverlay component
- ✅ Created CameraScreen with basic structure
- ✅ Added navigation and UI elements

### Partially Completed:
- ⚠️ Frame processing implementation (placeholder - needs actual frame capture)

### Next Steps:
1. Test current implementation on device
2. Implement proper frame capture from camera feed
3. Wire frame capture to pose detection service
4. Test pose detection accuracy and performance
5. Optimize for target 25-30 FPS

### Known Issues:
- Frame processing not yet implemented - camera will display but pose detection won't run
- Need to research best method for frame capture (expo-gl texture vs alternatives)
- Model loading time may be slow on first launch (consider caching)

### Files Created/Modified:
**New Files:**
- `src/types/pose.ts`
- `src/services/poseDetection.ts`
- `src/components/PoseOverlay.tsx`
- `src/screens/CameraScreen.tsx`

**Modified Files:**
- `App.tsx` - Added Camera route
- `src/screens/HomeScreen.tsx` - Added navigation button
- `package.json` - Added react-native-svg dependency

---

## Testing Plan

**Before Testing:**
- Verify all files compile without errors
- Check that dependencies are installed
- Ensure camera permissions are configured in app.json

**Testing Steps:**
1. Start Expo development server
2. Open app on physical device (camera doesn't work in simulator)
3. Navigate to Camera screen from Home
4. Verify camera feed displays
5. Check that model loads (loading indicator should disappear)
6. Verify FPS counter appears (will show 0 until frame processing is implemented)
7. Test error handling (if model fails to load)

**Expected Behavior:**
- Camera feed should display smoothly
- Model should load within 5-10 seconds
- FPS counter should appear in top-left
- No crashes or errors

**Known Limitations:**
- Pose detection won't work yet (frame processing not implemented)
- FPS will show 0 until frame capture is working
