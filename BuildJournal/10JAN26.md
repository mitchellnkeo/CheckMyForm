# Build Journal - January 10, 2026

In this build journal, document all steps done with reasons why those steps were taken. Ensure that everything document has the date and time.

---

## Phase 1: Pose Detection Foundation - Frame Processing Implementation

**Date:** January 10, 2026  
**Time:** Starting ~22:00 PST

### Objective
Implement frame capture from camera feed to enable real-time pose detection. Currently, the camera displays but no frames are being processed, so pose detection doesn't work.

### Current Status
- ✅ Camera feed displays
- ✅ Model loads successfully
- ✅ Pose detection service ready
- ✅ Visualization components ready
- ❌ Frame processing not implemented (placeholder)
- ❌ No pose detection happening

### Goal
Get pose detection working by capturing frames from the camera and processing them through the MoveNet model.

---

### Step 1: Attempt Frame Processing Implementation
**Time:** ~22:00 PST

**Action:** Attempted to implement frame capture from camera using `takePictureAsync`.

**Challenge Encountered:**
- Browser APIs (document.createElement, createImageBitmap, Image constructor) don't work in React Native
- Need React Native compatible method to convert image to TensorFlow tensor
- `tf.browser.fromPixels` requires browser DOM elements which don't exist in React Native

**Current Status:**
- Frame capture structure in place
- Image resizing using expo-image-manipulator
- Tensor conversion not yet implemented (needs React Native compatible solution)

**Next Steps:**
- Research React Native compatible image-to-tensor conversion
- Consider using expo-gl texture for better performance
- Or find/implement a React Native image-to-tensor utility

**Files Modified:**
- `src/screens/CameraScreen.tsx` - Added frame processing structure (incomplete)

**Note:** This is a complex problem. Frame processing in React Native with TensorFlow.js requires special handling. May need to use expo-gl or a custom solution.

---

## Current Status & Challenge

**Time:** ~22:05 PST

### The Problem
Converting camera frames to TensorFlow tensors in React Native is more complex than in web browsers because:
1. No DOM APIs (no canvas, Image elements, etc.)
2. `tf.browser.fromPixels` requires browser DOM elements
3. React Native Image component doesn't provide direct pixel access
4. Need native bridge or alternative approach

### Research Needed
1. **expo-gl texture approach** - Use WebGL texture from camera, then convert to tensor
2. **react-native-image-to-tensor** - Check if library exists
3. **Custom native module** - Write native code to extract pixels
4. **Alternative ML framework** - Consider React Native Vision Camera or similar

### Current Implementation Status
- ✅ Frame capture structure in place
- ✅ Image resizing working (expo-image-manipulator)
- ❌ Tensor conversion not implemented (blocking issue)
- ⚠️ Frame processing rate reduced to 5 FPS (from 30 FPS target)

### Next Steps
1. Research React Native + TensorFlow.js image-to-tensor solutions
2. Consider using expo-gl with camera texture
3. Or evaluate alternative camera/ML libraries
4. Implement working solution once approach is determined

**Decision Point:** Need to choose the best approach before continuing implementation.

---

## Research: Solutions for Image-to-Tensor Conversion in React Native

**Date:** January 10, 2026  
**Time:** ~22:10 PST

### Research Findings

After researching how developers handle TensorFlow.js image-to-tensor conversion in React Native, here are the key findings:

### Key Discovery: Most Developers Use Alternative Approaches

**Finding:** Many React Native developers working on pose detection are **not using TensorFlow.js with Expo Camera**. Instead, they're using:

1. **React Native Vision Camera + ML Kit** (Most Popular)
   - Uses native ML Kit Pose Detection
   - Frame processors work natively
   - Better performance than TensorFlow.js
   - Libraries: `react-native-vision-camera-v3-pose-detection`, `react-native-mlkit-pose-detection`

2. **MediaPipe React Native** (Second Most Popular)
   - Native MediaPipe integration
   - Optimized for real-time performance
   - Libraries: `@thinksys/react-native-mediapipe`, `react-native-mediapipe`

3. **TensorFlow Lite for React Native** (Alternative to TensorFlow.js)
   - Native TensorFlow Lite (not TensorFlow.js)
   - Better mobile performance
   - Library: `tflite-react-native-alternative`

### Why TensorFlow.js is Challenging in React Native

**The Core Problem:**
- TensorFlow.js was designed for web browsers
- `tf.browser.fromPixels()` requires DOM elements (canvas, Image, video)
- React Native doesn't have DOM APIs
- Converting images to tensors requires workarounds

**Why Alternatives Are Preferred:**
- Native solutions are faster (no JS bridge overhead)
- Better real-time performance
- Frame processors work directly with camera
- No image-to-tensor conversion needed (handled natively)

### Options for Our Project

#### Option 1: Continue with TensorFlow.js (Current Approach)
**Pros:**
- Already have MoveNet Lightning model working
- Model is loaded and ready
- TypeScript types and service layer complete

**Cons:**
- Image-to-tensor conversion is complex
- May need expo-gl texture (complex implementation)
- Slower performance than native solutions
- More workarounds needed

**Implementation Complexity:** High
**Performance:** Medium (5-10 FPS achievable)

#### Option 2: Switch to React Native Vision Camera + ML Kit
**Pros:**
- Native performance (30+ FPS)
- Frame processors work seamlessly
- No image-to-tensor conversion needed
- Well-documented and maintained
- Used by many production apps

**Cons:**
- Need to switch from Expo Camera
- Different API (ML Kit vs MoveNet)
- May need to adjust keypoint format
- Requires native build (not Expo Go compatible)

**Implementation Complexity:** Medium
**Performance:** High (30+ FPS)

#### Option 3: Use MediaPipe React Native
**Pros:**
- Native performance
- Real-time optimized
- Good documentation
- Cross-platform

**Cons:**
- Different model (not MoveNet)
- Need to switch libraries
- May have different keypoint format

**Implementation Complexity:** Medium
**Performance:** High (30+ FPS)

#### Option 4: Use TensorFlow Lite (Native)
**Pros:**
- Native performance
- Can use MoveNet model (converted to TFLite)
- Better than TensorFlow.js for mobile

**Cons:**
- Need to convert model format
- Different API
- More setup required

**Implementation Complexity:** High
**Performance:** High (30+ FPS)

### Recommendation

**For MVP/Portfolio Project:**
- **Option 2 (React Native Vision Camera + ML Kit)** is the best choice
- Most developers use this approach
- Better performance
- Easier implementation
- Production-ready

**For Learning/Staying with Current Stack:**
- Continue with TensorFlow.js but use expo-gl texture
- More complex but educational
- Can achieve 5-10 FPS which is acceptable for MVP

### Next Steps

1. **Decision:** Choose approach (recommend Option 2)
2. **If Option 2:** 
   - Install react-native-vision-camera
   - Install ML Kit pose detection plugin
   - Refactor CameraScreen to use Vision Camera
   - Adjust keypoint format if needed
3. **If Staying with TensorFlow.js:**
   - Research expo-gl texture implementation
   - Implement texture-to-tensor conversion
   - Accept lower FPS (5-10 FPS)

### Resources Found

- React Native Vision Camera: https://github.com/mrousavy/react-native-vision-camera
- ML Kit Pose Detection Plugin: https://github.com/gev2002/react-native-vision-camera-v3-pose-detection
- MediaPipe React Native: https://github.com/ThinkSys/mediapipe-reactnative
- TensorFlow.js React Native Tutorial: https://www.tensorflow.org/js/tutorials/applications/react_native

### Conclusion

The research shows that **most React Native developers use native solutions (ML Kit, MediaPipe) rather than TensorFlow.js** for pose detection because:
1. Better performance
2. Easier implementation
3. No image-to-tensor conversion challenges
4. Production-ready solutions

However, **continuing with TensorFlow.js is still viable** if we're willing to:
- Accept lower FPS (5-10 FPS)
- Implement expo-gl texture solution
- Use it as a learning experience

**Recommendation:** Switch to React Native Vision Camera + ML Kit for better results and easier implementation.

---

## Comparison: ML Kit vs TensorFlow Lite

**Date:** January 10, 2026  
**Time:** ~22:20 PST

### Question
Why use ML Kit instead of TensorFlow Lite? Both are native ML solutions.

### Detailed Comparison

#### ML Kit

**Pros:**
- ✅ **Easier to use** - Ready-to-use APIs, minimal setup
- ✅ **Pre-trained models** - Google handles model optimization
- ✅ **Cross-platform** - Works on both iOS and Android
- ✅ **Well-documented** - Good React Native integration
- ✅ **Automatic updates** - Google maintains and updates models
- ✅ **Less code** - Simple API calls

**Cons:**
- ❌ **Less customization** - Limited control over model
- ❌ **Different model** - Uses BlazePose (not MoveNet)
- ❌ **Different keypoint format** - May need to adjust code
- ❌ **Less control** - Can't fine-tune or customize model

**Best For:**
- Quick implementation
- Standard pose detection needs
- Developers without deep ML expertise
- When you don't need MoveNet specifically

#### TensorFlow Lite

**Pros:**
- ✅ **Use MoveNet** - Can use our existing MoveNet Lightning model
- ✅ **More control** - Full control over model and inference
- ✅ **Custom models** - Can use any TensorFlow Lite model
- ✅ **Fine-tuning** - Can optimize model for specific use case
- ✅ **Consistent with roadmap** - Roadmap specifies MoveNet

**Cons:**
- ❌ **More complex** - Requires more setup and configuration
- ❌ **More code** - Need to handle model loading, inference manually
- ❌ **Model conversion** - Need to convert MoveNet to TFLite format
- ❌ **More maintenance** - You manage model updates

**Best For:**
- When you need specific models (like MoveNet)
- Custom model requirements
- Maximum control and optimization
- When you want to use existing models

### For This Project Specifically

**Considerations:**
1. **Roadmap specifies MoveNet** - The roadmap explicitly mentions MoveNet Lightning
2. **We already have MoveNet** - Model is loaded and working (just need frame processing)
3. **Form analysis** - MoveNet's 17 keypoints match our analysis needs
4. **Portfolio value** - Using MoveNet shows understanding of model selection

**ML Kit would give us:**
- BlazePose model (33 keypoints vs MoveNet's 17)
- Easier implementation
- But different from roadmap specification

**TensorFlow Lite would give us:**
- MoveNet Lightning (as specified in roadmap)
- Keep existing model and code
- More complex but matches project goals

### Recommendation for This Project

**Actually, TensorFlow Lite might be better because:**
1. ✅ **Matches roadmap** - Roadmap specifies MoveNet Lightning
2. ✅ **Keep existing work** - We already have MoveNet model loaded
3. ✅ **Consistent keypoints** - 17 keypoints match our form analysis
4. ✅ **Portfolio alignment** - Shows we can work with specific models

**However, ML Kit is still valid if:**
- We're flexible on model choice
- Want faster implementation
- BlazePose's 33 keypoints are acceptable (actually more detailed)

### Updated Options

#### Option A: TensorFlow Lite + MoveNet (Recommended for this project)
- Use React Native Vision Camera for frame capture
- Use TensorFlow Lite to run MoveNet Lightning model
- Convert MoveNet to TFLite format
- Keep existing keypoint format (17 keypoints)

**Pros:** Matches roadmap, keeps MoveNet, consistent with project goals
**Cons:** More setup, need model conversion

#### Option B: ML Kit + BlazePose
- Use React Native Vision Camera for frame capture
- Use ML Kit Pose Detection (BlazePose model)
- Adjust keypoint format (33 keypoints instead of 17)

**Pros:** Easier implementation, well-documented
**Cons:** Different model, different keypoints, doesn't match roadmap

### Decision Framework

**Choose TensorFlow Lite if:**
- You want to stick with MoveNet (as per roadmap)
- You want to keep existing model work
- You're okay with more setup complexity
- Portfolio should show specific model selection

**Choose ML Kit if:**
- You want fastest implementation
- You're flexible on model choice
- BlazePose's 33 keypoints are acceptable (or preferred)
- You want Google-maintained solution

### Conclusion

For this specific project, **TensorFlow Lite + MoveNet** might actually be the better choice because:
1. It matches the roadmap specification
2. We can keep our existing MoveNet work
3. Shows ability to work with specific models
4. Maintains consistency with project goals

However, **ML Kit is still a valid choice** if we prioritize ease of implementation over strict adherence to the roadmap.

**Recommendation:** Use TensorFlow Lite with MoveNet to stay true to the roadmap and keep existing work, but ML Kit is acceptable if we want faster implementation.

---

## Decision: Switch to ML Kit

**Date:** January 10, 2026  
**Time:** ~22:30 PST

### Decision Made
User decided to proceed with ML Kit for the portfolio project, prioritizing:
- Easier implementation
- Cross-platform support (works on both iOS and Android)
- Production-ready solution
- Faster development

### Implementation Plan
1. Install React Native Vision Camera (for better frame processing)
2. Install ML Kit pose detection library
3. Refactor CameraScreen to use Vision Camera
4. Update pose types for ML Kit format (33 keypoints vs MoveNet's 17)
5. Update pose detection service
6. Update visualization components

### Packages to Install
- `react-native-vision-camera` - Better camera library with frame processors
- `react-native-mlkit-pose-detection` - ML Kit pose detection for React Native

### Next Steps
- Install packages
- Check Expo compatibility (may need development build)
- Refactor code to use ML Kit
- Test on device

---

## Step 1: Install Required Packages

**Time:** ~22:27 PST

**Action:** Installed React Native Vision Camera and ML Kit pose detection library.

**Packages Installed:**
- `react-native-vision-camera@^4.7.3` - High-performance camera library
- `react-native-mlkit-pose-detection@^0.1.8` - ML Kit pose detection for React Native

**Installation Method:** Used `npm install` with `--legacy-peer-deps` flag.

**Note:** `react-native-vision-camera` requires native modules and **does not work with Expo Go**. It requires a **development build** (EAS Build or `npx expo prebuild`). This is important to note for testing.

**Alternative Consideration:**
- Could use `react-native-mlkit-pose-detection` with Expo Camera instead
- Would work with Expo Go (no development build needed)
- But Expo Camera frame processing is limited
- Vision Camera has better frame processor support

**Files Modified:**
- `package.json` - Added new dependencies

**Next:** Need to decide on approach - Vision Camera (requires dev build) or Expo Camera + ML Kit (works with Expo Go)

---

## Step 2: Implement ML Kit Pose Detection Service

**Time:** ~22:35 PST

**Decision:** Using `react-native-mlkit-pose-detection` with Expo Camera (works with Expo Go, no dev build needed).

**Action:** Created ML Kit pose detection service and updated types.

### Changes Made:

1. **Updated `src/types/pose.ts`:**
   - Updated keypoint indices to match ML Kit's 33 keypoint format (BlazePose)
   - Mapped essential 17 keypoints from ML Kit's 33 keypoints
   - Added `MLKitKeypoint` interface for ML Kit's keypoint format

2. **Created `src/services/mlkitPoseDetection.ts`:**
   - New service class `MLKitPoseDetectionService`
   - `initialize()` - Initializes ML Kit (no model download needed)
   - `detectPose(imageUri)` - Detects pose from image URI
   - Converts ML Kit's 33 keypoints to our 17-keypoint format
   - Handles normalized coordinates (0-1) from ML Kit

3. **Updated `src/screens/CameraScreen.tsx`:**
   - Removed TensorFlow.js imports
   - Replaced `poseDetectionService` with `mlkitPoseDetectionService`
   - Updated initialization to use ML Kit
   - Simplified frame processing - ML Kit works directly with image URIs
   - Removed complex tensor conversion code
   - Updated loading message (no model download needed)

4. **Updated `src/components/PoseOverlay.tsx`:**
   - Added support for normalized coordinates (0-1)
   - Changed `modelInputSize` prop to `useNormalizedCoordinates` boolean
   - Scales normalized coordinates to screen size

### Key Differences from TensorFlow.js:

- **No model download:** ML Kit is built into the native SDK
- **Direct image URI:** Works with `file://` URIs from `takePictureAsync`
- **33 keypoints:** ML Kit provides more detailed pose data (we use 17)
- **Normalized coordinates:** Returns 0-1 range, need to scale to screen
- **Faster initialization:** No 30-60 second model download

### Files Modified:
- `src/types/pose.ts`
- `src/services/mlkitPoseDetection.ts` (new)
- `src/screens/CameraScreen.tsx`
- `src/components/PoseOverlay.tsx`

### Next Steps:
- Test on device to verify ML Kit API usage
- Adjust keypoint mapping if needed based on actual ML Kit output
- Optimize frame processing rate

**Note:** The ML Kit library API (`MLKitPoseDetection.detectPose()`) is based on typical ML Kit patterns. The actual API might differ slightly - we'll adjust based on runtime testing.

---

## Summary of ML Kit Migration

**Date:** January 10, 2026  
**Time:** ~22:40 PST

### What Was Accomplished:

1. ✅ **Installed ML Kit packages:**
   - `react-native-vision-camera` (for future optimization)
   - `react-native-mlkit-pose-detection` (for pose detection)

2. ✅ **Created ML Kit pose detection service:**
   - New service that uses ML Kit instead of TensorFlow.js
   - Handles 33 keypoints from BlazePose, maps to 17 essential keypoints
   - Works with image URIs (compatible with Expo Camera)

3. ✅ **Updated types and components:**
   - Updated pose types for ML Kit format
   - Updated CameraScreen to use ML Kit
   - Updated PoseOverlay to handle normalized coordinates

4. ✅ **Removed TensorFlow.js dependencies:**
   - No longer using TensorFlow.js for pose detection
   - Simplified frame processing (no tensor conversion needed)

### Key Benefits:

- **Faster initialization:** No 30-60 second model download
- **Works with Expo Go:** No development build required
- **Better performance:** Native ML Kit is optimized
- **Simpler code:** Direct image URI processing

### Testing Required:

- Verify ML Kit API calls work correctly
- Test pose detection accuracy
- Verify keypoint mapping is correct
- Test frame processing performance

---

## Issue: ML Kit Library Incompatibility

**Date:** January 10, 2026  
**Time:** ~22:45 PST

### Problem Encountered:

**Error:** `ReferenceError: Property '__SKRNMLKitPoseDetectionVisionCameraFrameProcessorPlugin' doesn't exist`

**Root Cause:**
- `react-native-mlkit-pose-detection` is designed **ONLY** for Vision Camera frame processors
- It does NOT support static image processing with Expo Camera
- The library requires Vision Camera and a Babel plugin configuration
- It's meant for real-time frame processing, not `takePictureAsync` image URIs

### Library Limitations:

1. **Vision Camera Only:** The library only works with `react-native-vision-camera` frame processors
2. **No Image URI Support:** Cannot process static images from `takePictureAsync`
3. **Requires Dev Build:** Vision Camera doesn't work with Expo Go (requires development build)
4. **Babel Plugin Required:** Needs special Babel configuration for frame processor

### Options Moving Forward:

**Option 1: Switch to Vision Camera (Requires Dev Build)**
- ✅ Better performance
- ✅ Real-time frame processing
- ✅ Works with ML Kit library
- ❌ Requires development build (not Expo Go compatible)
- ❌ More complex setup

**Option 2: Find Alternative ML Kit Library**
- Look for library that supports image URIs
- Check `@infinitered/react-native-mlkit-core` or similar
- May need to implement custom bridge

**Option 3: Go Back to TensorFlow Lite**
- Use `tflite-react-native` or `react-native-fast-tflite`
- Works with image URIs
- Requires model file management
- More setup but more control

**Option 4: Use TensorFlow.js with Different Approach**
- Use `expo-gl` texture for frame capture
- Convert frames to tensors properly
- More complex but works with Expo Go

### Immediate Fix:

- Made ML Kit service throw descriptive error
- Removed problematic import that causes startup crash
- App should now start without crashing (but pose detection won't work yet)

### Next Decision Needed:

User needs to decide:
1. Create development build and use Vision Camera + ML Kit
2. Find alternative ML Kit library for image URIs
3. Go back to TensorFlow Lite approach
4. Try TensorFlow.js with expo-gl texture approach

---

## Decision: Use Vision Camera + ML Kit (Option 1)

**Date:** January 10, 2026  
**Time:** ~22:50 PST

### Decision Made
User chose Option 1: Switch to Vision Camera + ML Kit for production-ready solution.

### Implementation Steps:

1. ✅ **Configured Babel for ML Kit frame processor:**
   - Updated `babel.config.js` to include ML Kit frame processor plugin
   - Added `__SKRNMLKitPoseDetectionVisionCameraFrameProcessorPlugin` to Reanimated globals

2. ✅ **Updated app.json:**
   - Added Vision Camera plugin configuration
   - Camera permissions already configured

3. ✅ **Rewrote ML Kit service:**
   - Updated to use Vision Camera frame processors
   - Changed from `detectPose(imageUri)` to `processFrame(frame)`
   - Uses `scanSKRNMLKitPose` from the library
   - Maps ML Kit's 33 keypoints to our 17-keypoint format

4. ✅ **Refactored CameraScreen:**
   - Replaced Expo Camera with Vision Camera
   - Uses `useCameraDevice` hook
   - Uses `useFrameProcessor` for real-time processing
   - Frame processor runs in worklet context for performance

5. ✅ **Created development build documentation:**
   - Added `DEVELOPMENT_BUILD.md` with setup instructions
   - Documented iOS and Android build steps
   - Added troubleshooting section

### Key Changes:

**Before (Expo Camera):**
- Used `takePictureAsync` (slow, ~200-500ms per frame)
- Processed at 5 FPS
- Static image processing

**After (Vision Camera):**
- Real-time frame processing (30+ FPS possible)
- Frame processor runs in worklet (native performance)
- Direct frame access, no image conversion needed

### Files Modified:

- `babel.config.js` - Added ML Kit frame processor plugin
- `app.json` - Added Vision Camera plugin
- `src/services/mlkitPoseDetection.ts` - Rewrote for Vision Camera
- `src/screens/CameraScreen.tsx` - Refactored to use Vision Camera
- `DEVELOPMENT_BUILD.md` - New file with build instructions

### Next Steps:

1. Run `npx expo prebuild` to generate native projects
2. Build for device: `npx expo run:ios` or `npx expo run:android`
3. Test pose detection on physical device
4. Adjust keypoint mapping if needed based on actual ML Kit output
5. Optimize frame processing rate if needed

---

## Prebuild Issue: Empty Asset Files

**Date:** January 10, 2026  
**Time:** ~23:00 PST

### Problem Encountered:

**Error:** `Error: [ios.dangerous]: withIosDangerousBaseMod: Could not find MIME for Buffer <null>`

**Root Cause:**
- Asset image files (`icon.png`, `splash.png`, `adaptive-icon.png`, `favicon.png`) were empty
- Expo prebuild tries to process these images for native projects
- Jimp library couldn't parse empty/null buffer

### Solution:

Created valid placeholder PNG images (1x1 pixel transparent PNGs) for all required assets:
- `assets/icon.png`
- `assets/splash.png`
- `assets/adaptive-icon.png`
- `assets/favicon.png`

**Note:** These are minimal placeholders. For production, replace with proper app icons and splash screens.

### Files Modified:
- `assets/icon.png` - Created placeholder
- `assets/splash.png` - Created placeholder
- `assets/adaptive-icon.png` - Created placeholder
- `assets/favicon.png` - Created placeholder

### Additional Issue: CocoaPods Encoding

**Problem:** After prebuild succeeded, CocoaPods installation failed due to encoding issue:
```
Unicode Normalization not appropriate for ASCII-8BIT (Encoding::CompatibilityError)
```

**Solution:** Set UTF-8 encoding before running `pod install`:
```bash
export LANG=en_US.UTF-8
cd ios
pod install
```

**Permanent Fix:** Add to `~/.zshrc`:
```bash
export LANG=en_US.UTF-8
```

Then reload: `source ~/.zshrc`

### Prebuild Success:

**Status:** ✅ Prebuild completed successfully
**CocoaPods:** ✅ Installed successfully (with UTF-8 encoding fix)

**Key Installations:**
- Vision Camera 4.7.3 - Frame Processors enabled ✅
- ML Kit Pose Detection 0.1.8 - Installed ✅
- React Native Worklets Core - Frame Processors enabled ✅
- All Expo modules installed ✅
- 113 total pods installed ✅

**Next Steps:**
1. Build for iOS: `npx expo run:ios`
2. Build for Android: `npx expo run:android`
3. Test pose detection on physical device

**Note:** Use `CheckMyFormAI.xcworkspace` (not `.xcodeproj`) for iOS development in Xcode.

---

## Build Error: Vision Camera 4.x Compatibility

**Date:** January 10, 2026  
**Time:** ~23:15 PST

### Problem Encountered:

**Error:** `VISION_EXPORT_FRAME_PROCESSOR: too few arguments provided to function-like macro invocation`

**Root Cause:**
- `react-native-mlkit-pose-detection` library was built for Vision Camera 2.x/3.x
- Vision Camera 4.x changed the `VISION_EXPORT_FRAME_PROCESSOR` macro signature
- Old macro: `VISION_EXPORT_FRAME_PROCESSOR(function_name)`
- New macro: `VISION_EXPORT_FRAME_PROCESSOR(class_name, plugin_name)`

### Solution:

Patched the library file to create a wrapper class that extends `FrameProcessorPlugin`:

1. Created `SKRNMLKitPoseDetectionFrameProcessorPlugin` class extending `FrameProcessorPlugin`
2. Implemented `callback:withArguments:` method (required by Vision Camera 4.x)
3. Updated macro call to use new signature with class and plugin name

**Files Modified:**
- `node_modules/react-native-mlkit-pose-detection/ios/SKRNMLKitPoseDetectionVisionCameraFrameProcessor.mm`

**Note:** This is a temporary patch. For production, consider:
- Using `patch-package` to persist the patch
- Finding an updated version of the library
- Contributing the fix back to the library

### Patch Details:

```objc
// Wrapper class for Vision Camera 4.x compatibility
@interface SKRNMLKitPoseDetectionFrameProcessorPlugin : FrameProcessorPlugin
@end

@implementation SKRNMLKitPoseDetectionFrameProcessorPlugin

- (id)callback:(Frame*)frame withArguments:(NSDictionary* _Nullable)arguments {
    SKRNMLKitPoseDetectionVisionCameraFrameProcessor *processor = [SKRNMLKitPoseDetectionVisionCameraFrameProcessor sharedInstance];
    return [processor poseResultsForVisionCameraFrame:frame];
}

@end

VISION_EXPORT_FRAME_PROCESSOR(SKRNMLKitPoseDetectionFrameProcessorPlugin, __SKRNMLKitPoseDetectionVisionCameraFrameProcessorPlugin);
```

### Patch Created:

Created a permanent patch using `patch-package`:
- `patches/react-native-mlkit-pose-detection+0.1.8.patch`
- Added `postinstall` script to `package.json` to apply patch automatically
- Patch will be applied after every `npm install`

**Next:** Try building again - the patch should resolve the Vision Camera 4.x compatibility issue.

### Implementation Details:

**Keypoint Mapping:**
- ML Kit uses capitalized keypoint names: "Nose", "LeftEyeInner", etc.
- Each keypoint has `position: { x, y, z }` and `inFrameLikelihood`
- Coordinates are in pixels, normalized to 0-1 for our overlay

**Frame Processor:**
- Uses `scanSKRNMLKitPose` directly in worklet context
- Converts ML Kit format to our 17-keypoint format
- Updates pose on JS thread using `runOnJS`

**Performance:**
- Frame processor runs in native worklet context (fast)
- Real-time processing at camera frame rate (30+ FPS possible)
- No image conversion overhead (direct frame access)

### Important Notes:

- **Requires development build** (not Expo Go compatible)
- First build will take longer (compiles native code)
- Must test on physical device (camera doesn't work in simulator)
- ML Kit keypoint names may need adjustment based on actual library output format

### Files Created/Modified:

**New Files:**
- `src/services/mlkitPoseDetection.ts`

**Modified Files:**
- `src/types/pose.ts`
- `src/screens/CameraScreen.tsx`
- `src/components/PoseOverlay.tsx`
- `package.json`
- `BuildJournal/10JAN26.md`

---

## ML Kit iOS Compatibility & Apple Device Concerns

**Date:** January 10, 2026  
**Time:** ~22:25 PST

### Question
ML Kit is made by Google - will it function the same on Apple devices? Is it proprietary?

### Answer: ML Kit Works on iOS, But There Are Considerations

**ML Kit iOS Support:**
- ✅ **Yes, ML Kit works on iOS** - It's cross-platform
- ✅ **Same APIs** - Same functionality on both platforms
- ✅ **On-device processing** - Works offline on both
- ⚠️ **iOS 15.5+ required** - Minimum iOS version
- ⚠️ **64-bit only** - No 32-bit support

**However, there are considerations:**

### Platform-Specific Considerations

**ML Kit on iOS:**
- Works well, but it's a Google product on Apple's platform
- May have slight performance differences (optimized for Android first)
- Requires Google dependencies on iOS
- Still fully functional and production-ready

**Apple's Native Alternative: Core ML**
- Apple's own ML framework
- Optimized for Apple Silicon
- Better integration with iOS ecosystem
- No Google dependencies

### Comparison: ML Kit vs Core ML on iOS

| Feature | ML Kit (Google) | Core ML (Apple) |
|---------|----------------|-----------------|
| **iOS Support** | ✅ Yes (iOS 15.5+) | ✅ Native (iOS 11+) |
| **Performance** | Good | Excellent (Apple-optimized) |
| **Cross-platform** | ✅ iOS + Android | ❌ iOS only |
| **Pose Detection** | ✅ BlazePose | ✅ Via custom models |
| **React Native Support** | ✅ Good | ⚠️ Requires native modules |
| **Ease of Use** | ✅ High-level APIs | ⚠️ More setup needed |

### For This Project

**If using ML Kit:**
- ✅ Works on both iOS and Android
- ✅ Same code for both platforms
- ✅ Well-documented React Native integration
- ⚠️ Google product on Apple platform (but fully supported)

**If using Core ML:**
- ✅ Native Apple solution
- ✅ Better iOS performance
- ✅ No Google dependencies
- ❌ iOS only (need separate Android solution)
- ⚠️ More complex React Native integration

### React Native Vision Camera + ML Kit

**Good news:** React Native Vision Camera works with both:
- **ML Kit** (cross-platform - iOS + Android)
- **Core ML** (iOS only, via custom models)

**For cross-platform apps, ML Kit is the standard choice** because:
1. Same code works on both platforms
2. Well-maintained React Native plugins
3. Production-ready and widely used
4. Performance is good on both platforms

### Recommendation

**ML Kit is fine for this project because:**
- ✅ It's cross-platform (works on iOS and Android)
- ✅ React Native Vision Camera has good ML Kit support
- ✅ Many production apps use ML Kit on iOS
- ✅ Performance is acceptable on both platforms
- ✅ Easier than maintaining separate iOS/Android solutions

**However, if you're concerned:**
- **TensorFlow Lite is platform-agnostic** (works on both, not tied to Google or Apple)
- **Core ML is Apple-native** but iOS-only (would need separate Android solution)

### Final Recommendation

**For a cross-platform React Native app:**
- **ML Kit** is the standard choice (works on both platforms)
- **TensorFlow Lite** is also good (platform-agnostic, can use MoveNet)
- **Core ML** only if iOS-only app (not suitable for cross-platform)

**ML Kit being "Google-proprietary" is not a problem** - it's fully supported on iOS and widely used in production apps. However, if you prefer a more neutral solution, **TensorFlow Lite is platform-agnostic** and might be a better fit.

---

## Clarification: Is ML Kit Still "ML"?

**Date:** January 10, 2026  
**Time:** ~22:15 PST

### Question
If we switch to React Native Vision Camera + ML Kit, will we still be using ML practices?

### Answer: YES - ML Kit is Still Machine Learning

**ML Kit is Google's Machine Learning SDK for Mobile:**
- Uses ML models under the hood (similar to MoveNet/BlazePose)
- Implements pose detection using neural networks
- Provides keypoints, confidence scores, and pose tracking
- All the same ML concepts, just a different API

### What Makes It ML:

1. **Uses Neural Network Models**
   - ML Kit Pose Detection uses trained ML models
   - Similar architecture to MoveNet (likely BlazePose-based)
   - Still doing inference on camera frames

2. **Same ML Concepts**
   - Keypoint detection (17 keypoints like MoveNet)
   - Confidence scores
   - Pose tracking over time
   - Real-time inference

3. **Still Demonstrates ML Skills**
   - Understanding when to use ML vs traditional computer vision
   - Working with ML model outputs
   - Processing keypoints and calculating angles
   - Form analysis from ML predictions

### The Difference:

**TensorFlow.js:**
- JavaScript implementation
- Runs in JS runtime
- You manage model loading, inference, tensor operations
- More control, more complexity

**ML Kit:**
- Native implementation (C++/native code)
- Runs on device natively
- Google handles model loading, inference
- Less control, easier to use
- Better performance

### Portfolio Value:

**Both approaches demonstrate:**
- ✅ Understanding of ML for computer vision
- ✅ Real-time pose detection implementation
- ✅ Working with ML model outputs
- ✅ Form analysis from ML predictions
- ✅ Production-ready ML integration

**ML Kit actually shows:**
- ✅ Understanding of choosing the right tool for the job
- ✅ Knowledge of native ML frameworks
- ✅ Production-ready ML practices
- ✅ Better performance optimization

### Industry Perspective:

**Most production apps use native ML frameworks:**
- ML Kit (Google)
- Core ML (Apple)
- TensorFlow Lite (native)
- MediaPipe (native)

**TensorFlow.js is more common for:**
- Web applications
- Prototyping
- Learning/education
- When you need JS runtime

### Conclusion:

**Yes, ML Kit is absolutely still ML!** It's actually a more standard approach for mobile ML applications. You're still:
- Using machine learning models
- Processing pose detection
- Working with ML outputs
- Building an ML-powered application

The only difference is the framework/API - the ML concepts and skills are the same. In fact, using ML Kit shows better understanding of production ML practices for mobile.
